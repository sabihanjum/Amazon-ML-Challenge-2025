{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXWH9s7LRVwy",
        "outputId": "5138f71d-42b6-4f0f-e0c6-7f351ab95a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import lightgbm as lgb\n",
        "import pickle"
      ],
      "metadata": {
        "id": "ma1WC1hHSHW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smape(y_true, y_pred):\n",
        "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "    return np.mean(numerator / denominator) * 100"
      ],
      "metadata": {
        "id": "Px_7-W2OST7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Read CSV with robust error handling for malformed rows\n",
        "try:\n",
        "    # First try: Standard read\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "except Exception as e:\n",
        "    print(f\"Standard read failed: {e}\")\n",
        "    print(\"Trying alternative parsing methods...\\n\")\n",
        "\n",
        "    try:\n",
        "        # Second try: Python engine with quote handling\n",
        "        train_df = pd.read_csv('train.csv',\n",
        "                               engine='python',\n",
        "                               on_bad_lines='skip',\n",
        "                               quoting=3,  # QUOTE_NONE\n",
        "                               sep='\\t')\n",
        "        # If tab-separated didn't work, try comma\n",
        "        if train_df.shape[1] == 1:\n",
        "            train_df = pd.read_csv('train.csv',\n",
        "                                   engine='python',\n",
        "                                   on_bad_lines='skip')\n",
        "    except:\n",
        "        try:\n",
        "            # Third try: Most robust - skip bad lines and warn\n",
        "            train_df = pd.read_csv('train.csv',\n",
        "                                   on_bad_lines='warn',\n",
        "                                   engine='python',\n",
        "                                   quotechar='\"',\n",
        "                                   escapechar='\\\\')\n",
        "        except:\n",
        "            # Final try: Read line by line\n",
        "            print(\"Using line-by-line reading (slowest but most robust)...\")\n",
        "            import csv\n",
        "\n",
        "            rows = []\n",
        "            with open('train.csv', 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                reader = csv.reader(f, quotechar='\"', escapechar='\\\\')\n",
        "                header = next(reader)\n",
        "\n",
        "                for i, row in enumerate(reader):\n",
        "                    try:\n",
        "                        if len(row) == len(header):\n",
        "                            rows.append(row)\n",
        "                    except:\n",
        "                        print(f\"Skipping malformed row {i+1}\")\n",
        "                        continue\n",
        "\n",
        "            train_df = pd.DataFrame(rows, columns=header)\n",
        "            # Convert price to numeric\n",
        "            train_df['price'] = pd.to_numeric(train_df['price'], errors='coerce')\n",
        "\n",
        "print(f\"\\n✓ Data loaded successfully!\")\n",
        "print(f\"Shape: {train_df.shape}\")\n",
        "print(f\"Price range: ${train_df['price'].min():.2f} - ${train_df['price'].max():.2f}\")\n",
        "print(f\"Median price: ${train_df['price'].median():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3uKRAhoSjBF",
        "outputId": "bc586ba8-0050-4b55-9e27-aafc1545f01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LOADING DATA\n",
            "================================================================================\n",
            "\n",
            "✓ Data loaded successfully!\n",
            "Shape: (75000, 4)\n",
            "Price range: $0.13 - $2796.00\n",
            "Median price: $14.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def extract_features(df):\n",
        "    \"\"\"Extract comprehensive features optimized for price prediction\"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "    df['catalog_content'] = df['catalog_content'].fillna('')\n",
        "\n",
        "    # ========== CORE PRICING FEATURES (MOST IMPORTANT) ==========\n",
        "\n",
        "    # Extract Value\n",
        "    def extract_value(text):\n",
        "        match = re.search(r'Value:\\s*(\\d+\\.?\\d*)', str(text))\n",
        "        return float(match.group(1)) if match else 1.0\n",
        "\n",
        "    df['value'] = df['catalog_content'].apply(extract_value)\n",
        "\n",
        "    # Extract Unit\n",
        "    def extract_unit(text):\n",
        "        match = re.search(r'Unit:\\s*(.+?)(?:\\n|$)', str(text))\n",
        "        if match:\n",
        "            unit = match.group(1).strip().lower()\n",
        "            # Standardize units\n",
        "            if 'fl oz' in unit or 'fluid ounce' in unit:\n",
        "                return 'fl_oz'\n",
        "            elif 'oz' in unit or 'ounce' in unit:\n",
        "                return 'oz'\n",
        "            elif 'lb' in unit or 'pound' in unit:\n",
        "                return 'lb'\n",
        "            elif 'count' in unit or 'pack' in unit:\n",
        "                return 'count'\n",
        "            elif any(x in unit for x in ['ml', 'milliliter']):\n",
        "                return 'ml'\n",
        "            elif any(x in unit for x in ['l', 'liter']):\n",
        "                return 'liter'\n",
        "            elif any(x in unit for x in ['g', 'gram']):\n",
        "                return 'gram'\n",
        "            elif any(x in unit for x in ['kg', 'kilo']):\n",
        "                return 'kg'\n",
        "            else:\n",
        "                return 'other'\n",
        "        return 'unknown'\n",
        "\n",
        "    df['unit'] = df['catalog_content'].apply(extract_unit)\n",
        "\n",
        "    # Extract Pack Quantity (CRITICAL)\n",
        "    def extract_pack_qty(text):\n",
        "        text = str(text).lower()\n",
        "        patterns = [\n",
        "            r'pack of (\\d+)', r'\\(pack of (\\d+)\\)', r'(\\d+)\\s*pack',\n",
        "            r'case of (\\d+)', r'(\\d+)\\s*per case', r'box of (\\d+)',\n",
        "            r'set of (\\d+)', r'(\\d+)\\s*count[^\\w]'\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                qty = int(match.group(1))\n",
        "                if 1 <= qty <= 500:\n",
        "                    return qty\n",
        "        return 1\n",
        "\n",
        "    df['pack_qty'] = df['catalog_content'].apply(extract_pack_qty)\n",
        "\n",
        "    # ========== KEY CALCULATED FEATURES ==========\n",
        "\n",
        "    # Unit price (most predictive feature)\n",
        "    df['value_per_pack'] = df['value'] / df['pack_qty']\n",
        "    df['log_value'] = np.log1p(df['value'])\n",
        "    df['log_pack_qty'] = np.log1p(df['pack_qty'])\n",
        "    df['log_value_per_pack'] = np.log1p(df['value_per_pack'])\n",
        "\n",
        "    # Standardize value to ounces for comparison\n",
        "    def value_to_oz(row):\n",
        "        val = row['value']\n",
        "        unit = row['unit']\n",
        "        if unit == 'fl_oz' or unit == 'oz':\n",
        "            return val\n",
        "        elif unit == 'lb':\n",
        "            return val * 16\n",
        "        elif unit == 'gram':\n",
        "            return val * 0.035274\n",
        "        elif unit == 'kg':\n",
        "            return val * 35.274\n",
        "        elif unit == 'ml':\n",
        "            return val * 0.033814\n",
        "        elif unit == 'liter':\n",
        "            return val * 33.814\n",
        "        else:\n",
        "            return val\n",
        "\n",
        "    df['value_in_oz'] = df.apply(value_to_oz, axis=1)\n",
        "    df['log_value_in_oz'] = np.log1p(df['value_in_oz'])\n",
        "    df['value_in_oz_per_pack'] = df['value_in_oz'] / df['pack_qty']\n",
        "    df['log_value_in_oz_per_pack'] = np.log1p(df['value_in_oz_per_pack'])\n",
        "\n",
        "    # ========== TEXT FEATURES ==========\n",
        "\n",
        "    df['text_length'] = df['catalog_content'].str.len()\n",
        "    df['word_count'] = df['catalog_content'].str.split().str.len()\n",
        "    df['num_bullet_points'] = df['catalog_content'].str.count('Bullet Point')\n",
        "    df['has_product_desc'] = df['catalog_content'].str.contains('Product Description').astype(int)\n",
        "\n",
        "    # Extract item name\n",
        "    def extract_item_name(text):\n",
        "        match = re.search(r'Item Name:\\s*(.+?)(?:\\n|Bullet Point|Value:|$)', str(text), re.IGNORECASE)\n",
        "        return match.group(1).strip() if match else ''\n",
        "\n",
        "    df['item_name'] = df['catalog_content'].apply(extract_item_name)\n",
        "    df['item_name_length'] = df['item_name'].str.len()\n",
        "    df['item_name_words'] = df['item_name'].str.split().str.len()\n",
        "\n",
        "    # ========== PREMIUM/QUALITY INDICATORS ==========\n",
        "\n",
        "    premium_keywords = {\n",
        "        'organic': 3.0, 'non-gmo': 2.5, 'natural': 2.0, 'premium': 2.5,\n",
        "        'gourmet': 3.0, 'artisan': 3.5, 'gluten free': 1.5, 'kosher': 1.5,\n",
        "        'vegan': 1.5, 'certified': 2.0, 'pure': 1.8, 'authentic': 1.5,\n",
        "        'fresh': 1.3, 'finest': 2.0, 'deluxe': 2.5, 'professional': 2.8\n",
        "    }\n",
        "\n",
        "    def calculate_premium_score(text):\n",
        "        text = str(text).lower()\n",
        "        score = 0\n",
        "        for keyword, weight in premium_keywords.items():\n",
        "            if keyword in text:\n",
        "                score += weight * text.count(keyword)\n",
        "        return score\n",
        "\n",
        "    df['premium_score'] = df['catalog_content'].apply(calculate_premium_score)\n",
        "    df['log_premium_score'] = np.log1p(df['premium_score'])\n",
        "\n",
        "    # Specific premium flags\n",
        "    for kw in ['organic', 'non_gmo', 'gluten_free', 'kosher', 'vegan']:\n",
        "        df[f'has_{kw}'] = df['catalog_content'].str.lower().str.contains(kw.replace('_', '-')).astype(int)\n",
        "\n",
        "    # ========== CATEGORY DETECTION ==========\n",
        "\n",
        "    categories = {\n",
        "        'beverage': ['drink', 'beverage', 'juice', 'water', 'soda', 'coffee', 'tea'],\n",
        "        'supplement': ['vitamin', 'supplement', 'protein', 'nutrition', 'powder'],\n",
        "        'condiment': ['sauce', 'seasoning', 'spice', 'dressing', 'marinade', 'oil', 'vinegar'],\n",
        "        'snack': ['chip', 'cookie', 'candy', 'chocolate', 'bar', 'cracker', 'popcorn'],\n",
        "        'canned': ['canned', 'can', 'jar', 'beans', 'soup'],\n",
        "        'baking': ['flour', 'sugar', 'baking', 'mix'],\n",
        "        'cereal': ['cereal', 'granola', 'oat', 'flake']\n",
        "    }\n",
        "\n",
        "    for cat, keywords in categories.items():\n",
        "        df[f'cat_{cat}'] = df['catalog_content'].str.lower().apply(\n",
        "            lambda x: int(any(kw in x for kw in keywords))\n",
        "        )\n",
        "\n",
        "    # ========== NUMBER EXTRACTION ==========\n",
        "\n",
        "    def extract_numbers(text):\n",
        "        return [float(x) for x in re.findall(r'\\d+\\.?\\d*', str(text)) if 0 < float(x) < 100000]\n",
        "\n",
        "    df['all_numbers'] = df['catalog_content'].apply(extract_numbers)\n",
        "    df['num_count'] = df['all_numbers'].apply(len)\n",
        "    df['max_number'] = df['all_numbers'].apply(lambda x: max(x) if x else 0)\n",
        "    df['min_number'] = df['all_numbers'].apply(lambda x: min(x) if x else 0)\n",
        "    df['mean_number'] = df['all_numbers'].apply(lambda x: np.mean(x) if x else 0)\n",
        "    df['median_number'] = df['all_numbers'].apply(lambda x: np.median(x) if x else 0)\n",
        "\n",
        "    # ========== BRAND/MANUFACTURER INDICATORS ==========\n",
        "\n",
        "    # Well-known value brands\n",
        "    value_brands = ['amazon', 'kirkland', 'great value', 'equate']\n",
        "    df['is_value_brand'] = df['catalog_content'].str.lower().apply(\n",
        "        lambda x: int(any(brand in x for brand in value_brands))\n",
        "    )\n",
        "\n",
        "    # ========== INTERACTION FEATURES ==========\n",
        "\n",
        "    df['value_x_premium'] = df['value'] * df['premium_score']\n",
        "    df['pack_x_premium'] = df['pack_qty'] * df['premium_score']\n",
        "    df['value_per_pack_x_premium'] = df['value_per_pack'] * df['premium_score']\n",
        "    df['bullets_per_pack'] = df['num_bullet_points'] / (df['pack_qty'] + 1)\n",
        "    df['text_per_value'] = df['text_length'] / (df['value'] + 1)\n",
        "\n",
        "    df.drop('all_numbers', axis=1, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df = extract_features(train_df)\n",
        "\n",
        "# Select features\n",
        "feature_cols = [col for col in train_df.columns\n",
        "                if col not in ['sample_id', 'catalog_content', 'image_link', 'price',\n",
        "                               'item_name', 'unit']]\n",
        "\n",
        "# Encode unit\n",
        "le_unit = LabelEncoder()\n",
        "train_df['unit_encoded'] = le_unit.fit_transform(train_df['unit'])\n",
        "feature_cols.append('unit_encoded')\n",
        "\n",
        "print(f\"Total features: {len(feature_cols)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSEwHTBbSlsw",
        "outputId": "f3e16bf6-27fe-4256-864e-e9e8318ac607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FEATURE ENGINEERING\n",
            "================================================================================\n",
            "Total features: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TEXT VECTORIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Combine item name and full text\n",
        "text_data = train_df['item_name'].fillna('') + ' ' + train_df['catalog_content'].fillna('')\n",
        "\n",
        "# TF-IDF with optimal parameters for pricing\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=2000,\n",
        "    ngram_range=(1, 3),\n",
        "    min_df=3,\n",
        "    max_df=0.9,\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "tfidf_matrix = tfidf.fit_transform(text_data)\n",
        "print(f\"TF-IDF shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "# SVD reduction\n",
        "n_svd = 100\n",
        "svd = TruncatedSVD(n_components=n_svd, random_state=42)\n",
        "tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
        "print(f\"SVD variance explained: {svd.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'tfidf_{i}' for i in range(n_svd)])\n",
        "\n",
        "# ============================================================================\n",
        "# 6. PREPARE DATA\n",
        "# ============================================================================\n",
        "\n",
        "X_basic = train_df[feature_cols].fillna(0)\n",
        "X_combined = pd.concat([X_basic.reset_index(drop=True), tfidf_df], axis=1)\n",
        "y = train_df['price'].values\n",
        "\n",
        "# Log transform target\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "print(f\"\\nFinal feature matrix: {X_combined.shape}\")\n",
        "print(f\"Total features: {X_combined.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLzNFldES5bu",
        "outputId": "f5009cc9-dac8-4802-b1ac-f99cc1c09f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TEXT VECTORIZATION\n",
            "================================================================================\n",
            "TF-IDF shape: (75000, 2000)\n",
            "SVD variance explained: 0.397\n",
            "\n",
            "Final feature matrix: (75000, 142)\n",
            "Total features: 142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import lightgbm as lgb\n",
        "\n",
        "# --- SMAPE Function ---\n",
        "def smape(y_true, y_pred):\n",
        "    return 100 * np.mean(\n",
        "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)\n",
        "    )\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TRAINING LIGHTGBM (Target SMAPE < 40)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# --- Tuned LightGBM Parameters ---\n",
        "lgb_params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'mae',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 180,            # more leaves → higher model complexity\n",
        "    'learning_rate': 0.008,       # smaller → more precise fitting\n",
        "    'n_estimators': 8000,         # more boosting rounds with early stop\n",
        "    'feature_fraction': 0.85,     # use more features per split\n",
        "    'bagging_fraction': 0.85,     # larger sample usage\n",
        "    'bagging_freq': 3,\n",
        "    'min_child_samples': 10,\n",
        "    'min_child_weight': 0.0005,\n",
        "    'reg_alpha': 0.05,\n",
        "    'reg_lambda': 0.05,\n",
        "    'max_depth': -1,\n",
        "    'verbosity': -1,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "# --- KFold Setup ---\n",
        "n_splits = 5\n",
        "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "fold_scores = []\n",
        "oof_predictions = np.zeros(len(y))\n",
        "models = []\n",
        "\n",
        "print(\"\\nTraining folds...\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_combined), 1):\n",
        "    print(f\"\\nFold {fold}/{n_splits}\")\n",
        "\n",
        "    X_train, X_val = X_combined.iloc[train_idx], X_combined.iloc[val_idx]\n",
        "    y_train, y_val = y_log[train_idx], y_log[val_idx]\n",
        "    y_val_original = y[val_idx]\n",
        "\n",
        "    # --- Scaling ---\n",
        "    scaler = RobustScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    # --- Model ---\n",
        "    model = lgb.LGBMRegressor(**lgb_params)\n",
        "    model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        eval_set=[(X_val_scaled, y_val)],\n",
        "        eval_metric='mae',\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
        "            lgb.log_evaluation(period=250)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # --- Predictions ---\n",
        "    y_pred_log = model.predict(X_val_scaled, num_iteration=model.best_iteration_)\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    y_pred = np.maximum(y_pred, 0.01)\n",
        "\n",
        "    oof_predictions[val_idx] = y_pred\n",
        "\n",
        "    fold_smape = smape(y_val_original, y_pred)\n",
        "    fold_scores.append(fold_smape)\n",
        "\n",
        "    print(f\"  ✅ SMAPE: {fold_smape:.4f}%\")\n",
        "    print(f\"  📉 Best iteration: {model.best_iteration_}\")\n",
        "\n",
        "    models.append((model, scaler))\n",
        "\n",
        "mean_smape = np.mean(fold_scores)\n",
        "std_smape = np.std(fold_scores)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FINAL CROSS-VALIDATION RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nFold scores: {[f'{s:.4f}%' for s in fold_scores]}\")\n",
        "print(f\"\\nMean SMAPE: {mean_smape:.4f}% (+/- {std_smape:.4f}%)\")\n",
        "print(f\"OOF SMAPE: {smape(y, oof_predictions):.4f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAgr1-UreURK",
        "outputId": "76eb15bc-f7df-454d-a71d-eaff3ced1530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING LIGHTGBM (Target SMAPE < 40)\n",
            "================================================================================\n",
            "\n",
            "Training folds...\n",
            "\n",
            "Fold 1/5\n",
            "[250]\tvalid_0's l1: 0.591009\n",
            "[500]\tvalid_0's l1: 0.561013\n",
            "[750]\tvalid_0's l1: 0.548947\n",
            "[1000]\tvalid_0's l1: 0.542128\n",
            "[1250]\tvalid_0's l1: 0.538016\n",
            "[1500]\tvalid_0's l1: 0.534729\n",
            "[1750]\tvalid_0's l1: 0.532286\n",
            "[2000]\tvalid_0's l1: 0.530135\n",
            "[2250]\tvalid_0's l1: 0.528214\n",
            "[2500]\tvalid_0's l1: 0.526543\n",
            "[2750]\tvalid_0's l1: 0.525055\n",
            "[3000]\tvalid_0's l1: 0.523654\n",
            "[3250]\tvalid_0's l1: 0.522555\n",
            "[3500]\tvalid_0's l1: 0.521541\n",
            "[3750]\tvalid_0's l1: 0.520595\n",
            "[4000]\tvalid_0's l1: 0.519852\n",
            "[4250]\tvalid_0's l1: 0.519141\n",
            "[4500]\tvalid_0's l1: 0.518495\n",
            "[4750]\tvalid_0's l1: 0.517922\n",
            "[5000]\tvalid_0's l1: 0.517436\n",
            "[5250]\tvalid_0's l1: 0.517024\n",
            "[5500]\tvalid_0's l1: 0.516643\n",
            "[5750]\tvalid_0's l1: 0.51625\n",
            "[6000]\tvalid_0's l1: 0.515867\n",
            "[6250]\tvalid_0's l1: 0.515507\n",
            "[6500]\tvalid_0's l1: 0.515276\n",
            "[6750]\tvalid_0's l1: 0.514968\n",
            "[7000]\tvalid_0's l1: 0.514732\n",
            "[7250]\tvalid_0's l1: 0.514497\n",
            "[7500]\tvalid_0's l1: 0.514259\n",
            "[7750]\tvalid_0's l1: 0.514099\n",
            "[8000]\tvalid_0's l1: 0.513969\n",
            "  ✅ SMAPE: 51.6136%\n",
            "  📉 Best iteration: 7991\n",
            "\n",
            "Fold 2/5\n",
            "[250]\tvalid_0's l1: 0.57829\n",
            "[500]\tvalid_0's l1: 0.54688\n",
            "[750]\tvalid_0's l1: 0.534492\n",
            "[1000]\tvalid_0's l1: 0.527683\n",
            "[1250]\tvalid_0's l1: 0.523717\n",
            "[1500]\tvalid_0's l1: 0.520607\n",
            "[1750]\tvalid_0's l1: 0.518225\n",
            "[2000]\tvalid_0's l1: 0.516167\n",
            "[2250]\tvalid_0's l1: 0.514657\n",
            "[2500]\tvalid_0's l1: 0.51311\n",
            "[2750]\tvalid_0's l1: 0.511783\n",
            "[3000]\tvalid_0's l1: 0.510736\n",
            "[3250]\tvalid_0's l1: 0.509768\n",
            "[3500]\tvalid_0's l1: 0.508874\n",
            "[3750]\tvalid_0's l1: 0.508132\n",
            "[4000]\tvalid_0's l1: 0.507333\n",
            "[4250]\tvalid_0's l1: 0.506739\n",
            "[4500]\tvalid_0's l1: 0.506085\n",
            "[4750]\tvalid_0's l1: 0.505583\n",
            "[5000]\tvalid_0's l1: 0.505148\n",
            "[5250]\tvalid_0's l1: 0.504703\n",
            "[5500]\tvalid_0's l1: 0.504388\n",
            "[5750]\tvalid_0's l1: 0.504022\n",
            "[6000]\tvalid_0's l1: 0.503709\n",
            "[6250]\tvalid_0's l1: 0.503505\n",
            "[6500]\tvalid_0's l1: 0.50323\n",
            "[6750]\tvalid_0's l1: 0.503028\n",
            "[7000]\tvalid_0's l1: 0.502804\n",
            "[7250]\tvalid_0's l1: 0.502613\n",
            "[7500]\tvalid_0's l1: 0.502506\n",
            "[7750]\tvalid_0's l1: 0.502346\n",
            "[8000]\tvalid_0's l1: 0.502179\n",
            "  ✅ SMAPE: 50.5718%\n",
            "  📉 Best iteration: 8000\n",
            "\n",
            "Fold 3/5\n",
            "[250]\tvalid_0's l1: 0.579484\n",
            "[500]\tvalid_0's l1: 0.549383\n",
            "[750]\tvalid_0's l1: 0.537584\n",
            "[1000]\tvalid_0's l1: 0.530464\n",
            "[1250]\tvalid_0's l1: 0.526285\n",
            "[1500]\tvalid_0's l1: 0.523128\n",
            "[1750]\tvalid_0's l1: 0.520571\n",
            "[2000]\tvalid_0's l1: 0.518556\n",
            "[2250]\tvalid_0's l1: 0.517\n",
            "[2500]\tvalid_0's l1: 0.515575\n",
            "[2750]\tvalid_0's l1: 0.514196\n",
            "[3000]\tvalid_0's l1: 0.512981\n",
            "[3250]\tvalid_0's l1: 0.511954\n",
            "[3500]\tvalid_0's l1: 0.511061\n",
            "[3750]\tvalid_0's l1: 0.510276\n",
            "[4000]\tvalid_0's l1: 0.509474\n",
            "[4250]\tvalid_0's l1: 0.508774\n",
            "[4500]\tvalid_0's l1: 0.508163\n",
            "[4750]\tvalid_0's l1: 0.50754\n",
            "[5000]\tvalid_0's l1: 0.507074\n",
            "[5250]\tvalid_0's l1: 0.506566\n",
            "[5500]\tvalid_0's l1: 0.506227\n",
            "[5750]\tvalid_0's l1: 0.505854\n",
            "[6000]\tvalid_0's l1: 0.50549\n",
            "[6250]\tvalid_0's l1: 0.505147\n",
            "[6500]\tvalid_0's l1: 0.504877\n",
            "[6750]\tvalid_0's l1: 0.504623\n",
            "[7000]\tvalid_0's l1: 0.504461\n",
            "[7250]\tvalid_0's l1: 0.504313\n",
            "[7500]\tvalid_0's l1: 0.504084\n",
            "[7750]\tvalid_0's l1: 0.503854\n",
            "[8000]\tvalid_0's l1: 0.503685\n",
            "  ✅ SMAPE: 50.8929%\n",
            "  📉 Best iteration: 8000\n",
            "\n",
            "Fold 4/5\n",
            "[250]\tvalid_0's l1: 0.568777\n",
            "[500]\tvalid_0's l1: 0.537806\n",
            "[750]\tvalid_0's l1: 0.525999\n",
            "[1000]\tvalid_0's l1: 0.519806\n",
            "[1250]\tvalid_0's l1: 0.515475\n",
            "[1500]\tvalid_0's l1: 0.51262\n",
            "[1750]\tvalid_0's l1: 0.510005\n",
            "[2000]\tvalid_0's l1: 0.508184\n",
            "[2250]\tvalid_0's l1: 0.506511\n",
            "[2500]\tvalid_0's l1: 0.505122\n",
            "[2750]\tvalid_0's l1: 0.503848\n",
            "[3000]\tvalid_0's l1: 0.502736\n",
            "[3250]\tvalid_0's l1: 0.501806\n",
            "[3500]\tvalid_0's l1: 0.500921\n",
            "[3750]\tvalid_0's l1: 0.50001\n",
            "[4000]\tvalid_0's l1: 0.499325\n",
            "[4250]\tvalid_0's l1: 0.49872\n",
            "[4500]\tvalid_0's l1: 0.498087\n",
            "[4750]\tvalid_0's l1: 0.49761\n",
            "[5000]\tvalid_0's l1: 0.497143\n",
            "[5250]\tvalid_0's l1: 0.496702\n",
            "[5500]\tvalid_0's l1: 0.496255\n",
            "[5750]\tvalid_0's l1: 0.495924\n",
            "[6000]\tvalid_0's l1: 0.495629\n",
            "[6250]\tvalid_0's l1: 0.49527\n",
            "[6500]\tvalid_0's l1: 0.494983\n",
            "[6750]\tvalid_0's l1: 0.494783\n",
            "[7000]\tvalid_0's l1: 0.494573\n",
            "[7250]\tvalid_0's l1: 0.494394\n",
            "[7500]\tvalid_0's l1: 0.494249\n",
            "[7750]\tvalid_0's l1: 0.494081\n",
            "[8000]\tvalid_0's l1: 0.493948\n",
            "  ✅ SMAPE: 49.8834%\n",
            "  📉 Best iteration: 7995\n",
            "\n",
            "Fold 5/5\n",
            "[250]\tvalid_0's l1: 0.579349\n",
            "[500]\tvalid_0's l1: 0.548657\n",
            "[750]\tvalid_0's l1: 0.536826\n",
            "[1000]\tvalid_0's l1: 0.530042\n",
            "[1250]\tvalid_0's l1: 0.525991\n",
            "[1500]\tvalid_0's l1: 0.522829\n",
            "[1750]\tvalid_0's l1: 0.52055\n",
            "[2000]\tvalid_0's l1: 0.51851\n",
            "[2250]\tvalid_0's l1: 0.516704\n",
            "[2500]\tvalid_0's l1: 0.515185\n",
            "[2750]\tvalid_0's l1: 0.513735\n",
            "[3000]\tvalid_0's l1: 0.512587\n",
            "[3250]\tvalid_0's l1: 0.511506\n",
            "[3500]\tvalid_0's l1: 0.510701\n",
            "[3750]\tvalid_0's l1: 0.50984\n",
            "[4000]\tvalid_0's l1: 0.509021\n",
            "[4250]\tvalid_0's l1: 0.508373\n",
            "[4500]\tvalid_0's l1: 0.507673\n",
            "[4750]\tvalid_0's l1: 0.507203\n",
            "[5000]\tvalid_0's l1: 0.506753\n",
            "[5250]\tvalid_0's l1: 0.506257\n",
            "[5500]\tvalid_0's l1: 0.505738\n",
            "[5750]\tvalid_0's l1: 0.505357\n",
            "[6000]\tvalid_0's l1: 0.505029\n",
            "[6250]\tvalid_0's l1: 0.504752\n",
            "[6500]\tvalid_0's l1: 0.504485\n",
            "[6750]\tvalid_0's l1: 0.504259\n",
            "[7000]\tvalid_0's l1: 0.504052\n",
            "[7250]\tvalid_0's l1: 0.503868\n",
            "[7500]\tvalid_0's l1: 0.503657\n",
            "[7750]\tvalid_0's l1: 0.503471\n",
            "[8000]\tvalid_0's l1: 0.503331\n",
            "  ✅ SMAPE: 50.6613%\n",
            "  📉 Best iteration: 7998\n",
            "\n",
            "================================================================================\n",
            "FINAL CROSS-VALIDATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "Fold scores: ['51.6136%', '50.5718%', '50.8929%', '49.8834%', '50.6613%']\n",
            "\n",
            "Mean SMAPE: 50.7246% (+/- 0.5574%)\n",
            "OOF SMAPE: 50.7246%\n"
          ]
        }
      ]
    }
  ]
}